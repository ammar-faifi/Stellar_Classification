[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stellar Classification",
    "section": "",
    "text": "The Stellar Classification Dataset - SDSS17 consists of 100,000 observations with 17 features that allows for a stellar object to be classified as a star, galaxy or quasar.\nTarget variable : the stellar classification ( star, galaxy or quasar)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "the Stellar Classification Dataset - SDSS17data was obtained from kaggle\nthe dataset has the following attributes\n\n\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\n1\nobj_ID\nObject Identifier, the unique value that identifies the object in the image catalog used by the CAS\n\n\n2\nalpha\nRight Ascension angle (at J2000 epoch)\n\n\n3\ndelta\nDeclination angle (at J2000 epoch)\n\n\n4\nu\nUltraviolet filter in the photometric system\n\n\n5\ng\nGreen filter in the photometric system\n\n\n6\nr\nRed filter in the photometric system\n\n\n7\ni\nNear Infrared filter in the photometric system\n\n\n8\nz\nInfrared filter in the photometric system\n\n\n9\nrun_ID\nRun Number used to identify the specific scan\n\n\n10\nrereun_ID\nRerun Number to specify how the image was processed\n\n\n11\ncam_col\nCamera column to identify the scanline within the run\n\n\n12\nfield_ID\nField number to identify each field\n\n\n13\nspec_obj_ID\nUnique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n\n\n14\nclass\nobject class (galaxy, star or quasar object)\n\n\n15\nredshift\nredshift value based on the increase in wavelength\n\n\n16\nplate\nplate ID, identifies each plate in SDSS"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Stellar Classification",
    "section": "Data",
    "text": "Data\nthe Stellar Classification Dataset - SDSS17data was obtained from kaggle\nthe dataset has the following attributes\n\n\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\n1\nobj_ID\nObject Identifier, the unique value that identifies the object in the image catalog used by the CAS\n\n\n2\nalpha\nRight Ascension angle (at J2000 epoch)\n\n\n3\ndelta\nDeclination angle (at J2000 epoch)\n\n\n4\nu\nUltraviolet filter in the photometric system\n\n\n5\ng\nGreen filter in the photometric system\n\n\n6\nr\nRed filter in the photometric system\n\n\n7\ni\nNear Infrared filter in the photometric system\n\n\n8\nz\nInfrared filter in the photometric system\n\n\n9\nrun_ID\nRun Number used to identify the specific scan\n\n\n10\nrereun_ID\nRerun Number to specify how the image was processed\n\n\n11\ncam_col\nCamera column to identify the scanline within the run\n\n\n12\nfield_ID\nField number to identify each field\n\n\n13\nspec_obj_ID\nUnique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)\n\n\n14\nclass\nobject class (galaxy, star or quasar object)\n\n\n15\nredshift\nredshift value based on the increase in wavelength\n\n\n16\nplate\nplate ID, identifies each plate in SDSS"
  },
  {
    "objectID": "index.html#team-members",
    "href": "index.html#team-members",
    "title": "Stellar Classification",
    "section": "Team members",
    "text": "Team members\nAhmed Almohammed Turki Alsaedi Sultan Alkadhi Salman Al-Harbi Ammar Alfaifi Lana almorabah"
  },
  {
    "objectID": "KNN_and_XGB.html",
    "href": "KNN_and_XGB.html",
    "title": "KNN & XGB",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.dpi\"] = 170\nplt.style.use(\"seaborn\")\n\nimport xgboost as xgb\nfrom sklearn.model_selection import (\n    train_test_split,\n    KFold,\n    GridSearchCV,\n    cross_val_score,\n    RandomizedSearchCV,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import RocCurveDisplay\n\n# import warnings filter\nfrom warnings import simplefilter\n\n# ignore all future warnings\nsimplefilter(action=\"ignore\", category=FutureWarning)"
  },
  {
    "objectID": "KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "href": "KNN_and_XGB.html#basic-knn-with-5-fold-cv",
    "title": "KNN & XGB",
    "section": "Basic KNN with 5-Fold CV",
    "text": "Basic KNN with 5-Fold CV\n\n\nCode\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# create 5-Fold CV\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# Fit the model\nresults = cross_val_score(knn, X_train_std, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9255714285714286"
  },
  {
    "objectID": "KNN_and_XGB.html#tuning-the-knn-model",
    "href": "KNN_and_XGB.html#tuning-the-knn-model",
    "title": "KNN & XGB",
    "section": "Tuning the KNN Model",
    "text": "Tuning the KNN Model\n\nCode\n# hyper parameters for CV\nhyper_params = {\n    'n_neighbors': range(1, 10+1)\n}\n\nscaler = StandardScaler()\n# standardize all columns\nX_train_std = scaler.fit_transform(X_train)\n\n# create KNN model\nknn = KNeighborsClassifier()\n\n# create 5-Fold CV\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# Tune `knn` using grid search\ngrid_search = GridSearchCV(knn, hyper_params, cv=kfold, scoring='accuracy')\ngrid_results = grid_search.fit(X_train_std, y_train)\n\n\n\nCode\n# get the best accuracy achieved\nprint(\"Best accuracy\", grid_results.best_score_)\nprint(\"Best K value\", grid_results.best_estimator_.get_params()['n_neighbors'])\n\n\nBest accuracy 0.9271142857142858\nBest K value 3\n\n\n\n\nCode\nplt.plot(hyper_params['n_neighbors'], grid_search.cv_results_['mean_test_score'])\nplt.title('Cross validated grid search results'.title())\nplt.show()\n\n\n\n\n\n\nFeature Engineering\n\nCode\n%%capture\n\n# fit the KNN with best K value\nknn_best = KNeighborsClassifier(n_neighbors=3)\nknn_best_fit = knn.fit(X_train_std, y_train)\n\nr = permutation_importance(\n    knn,\n    X_train_std,\n    y_train,\n    n_repeats=5,\n    random_state=123,\n    n_jobs=-1,\n)\n\n\n\nCode\nfeat = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': r.importances_mean,\n}).sort_values('importance')\n\nplt.scatter(data=feat, x='importance', y='feature')\nplt.xlabel(\"Importance\")\nplt.title('feature interpretation'.title())\nplt.show()"
  },
  {
    "objectID": "KNN_and_XGB.html#basic-gradient-boosting-model",
    "href": "KNN_and_XGB.html#basic-gradient-boosting-model",
    "title": "KNN & XGB",
    "section": "Basic Gradient Boosting Model",
    "text": "Basic Gradient Boosting Model\n\n\nCode\n# create XGBClassifier model\nxgb_model = xgb.XGBClassifier()\n\n# create 5-Fold CV\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# Fit the model\nresults = cross_val_score(xgb_model, X_train, y_train, cv=kfold, scoring='accuracy')\n\nprint(max(results))\n\n\n0.9791428571428571\n\n\n\nWith Randomized Searching\n\nCode\nparam_distributions = {\n    'n_estimators': [1000, 2500, 5000],\n    'learning_rate': [0.001, 0.01, 0.1],\n    'max_depth': [3, 5, 7, 9],\n    'min_child_weight': [1, 5, 15]\n}\n\nrandom_search = RandomizedSearchCV(\n    xgb_model,\n    param_distributions,\n    n_iter=5,\n    cv=kfold,\n    scoring='accuracy',\n    random_state=123,\n    n_jobs=-1,\n)\n\nsearch_results = random_search.fit(X_train_std, y_train)\n\n\n\nCode\nsearch_results.best_score_\n\n\n0.9789571428571427"
  },
  {
    "objectID": "LogisticRegression.html",
    "href": "LogisticRegression.html",
    "title": "LogisticRegression",
    "section": "",
    "text": "Code\n#####lana \n\nplt.figure(figsize=(13,7))\nsns.heatmap(df.corr(),annot=True,vmin=-1,vmax=1)\n\n\nCode\n# read the dataset\ndf = pd.read_csv(\"archive\\star_classification.csv\")\n\n# encode values for class column\ndf.replace({'class': {'GALAXY': 0, 'STAR': 1, 'QSO':2}}, inplace=True)\n\n# # remove all columns containing ID at the end\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# # drop the date column\n# cleaned = cleaned.drop(['MJD',\"plate\"], axis=1)\n# cleaned=cleaned.drop(79543)\n# # make the X and y varialbes\n# X = cleaned.drop('class', axis=1)\n# y = cleaned['class']\n\n# # split the dataset\n# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\nCode\ndf\n\n\nCode\ndf.corr().style.background_gradient(cmap=\"coolwarm\")\n\n\nCode\ndf.corr()[\"class\"].sort_values()\n\n\nCode\nfrom dis import dis\ncleaned = df.drop(['u ','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"u\",\"r\",\"z\",\"alpha\",\"delta\",\"MJD\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\n\nCode\nsns.displot(df[\"run_ID\"]);\n\n\nCode\nsns.displot(df[\"field_ID\"]);\n\n\nCode\nsns.displot(df[\"MJD\"]);\n\n\nCode\ncleaned[cleaned[\"redshift\"]>5]\n\n\nCode\nsns.displot(cleaned[\"alpha\"]);\n\n\nCode\nsns.displot(cleaned[\"delta\"]);\n\n\nCode\nsns.displot(x=cleaned[\"u\"]);\n\n\nCode\ncleaned\n\n\nCode\nsns.displot(x=cleaned[\"g\"]);\n\n\nCode\nsns.displot(cleaned[\"r\"]);\n\n\nCode\nsns.displot(cleaned[\"i\"]);\n\n\nCode\nsns.displot(cleaned[\"z\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"cam_col\"]);\n\n\nCode\nsns.countplot(x=cleaned[\"class\"]);\n\n\nCode\nsns.displot(cleaned[\"redshift\"]);\n\n\nCode\nsns.displot(cleaned[\"plate\"]);\n\n\npre-processing\n\nCode\n# Normalizing approach\nyj = PowerTransformer(method=\"yeo-johnson\")\nscaler = StandardScaler()\n# nzv_encoder = VarianceThreshold(threshold=0.1)\n# pca = PCA(n_components=7)\n# Normalize all numeric features\npreprocessor = ColumnTransformer([(\"norm\", yj, selector(dtype_include=\"number\")),\n                (\"std_encode\", scaler, selector(dtype_include=\"number\")),\n                # (\"nzv_encoder\", nzv_encoder, selector(dtype_include=\"number\")),\n                # (\"pca_encode\", pca, selector(dtype_include=\"number\"))\n                ])\n\n\n\nRandom Forest Classifier\n\nCode\nr_forest = RandomForestClassifier()\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", r_forest),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n0.983\n\n\nCode\nprint(classification_report(y_test, predicted))\n\n\n\nSVM\n\nCode\nsvm_clf = svm.SVC(kernel='rbf', C=32, random_state=0)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", svm_clf),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n#0.977\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\n\nLogistic Regression\n\nCode\nlog_reg=LogisticRegression()\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"log_reg\", log_reg),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\n# results = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\n# results\n\n\nCode\nresults.mean()\n\n\nCode\n# Create grid of hyperparameter values\nhyper_grid = {'log_reg__max_iter': range(100, 1000,100)}\n\n# Tune a knn model using grid search\ngrid_search = GridSearchCV(model_pipeline, hyper_grid, cv=kfold, scoring=loss)\nresults = grid_search.fit(X_train, y_train)\n\n\nCode\nresults.best_params_\n\n\n\nK-Nearest Neighbors\n\nCode\n\nknn=KNeighborsClassifier(n_neighbors=3)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"knn\", knn),\n])\nmodel_pipeline.fit(X_train,y_train)\npredicted = model_pipeline.predict(X_test)\nscore = model_pipeline.score(X_test,y_test)\nmodel_pipeline_score = np.mean(score)\nmodel_pipeline_score\n\n\nCode\nprint(classification_report(y_test, predicted)) \n\n\n\ndeep learining\n\nCode\nfrom dis import dis\n\n# cleaned = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)\ncleaned = df.drop(['u','r','i','z','obj_ID','spec_obj_ID','MJD'], axis = 1)\n\n# cleaned = df.drop(df.filter(regex='ID$').columns, axis=1)\n# drop the date column\n# cleaned = cleaned.drop([\"MJD\",\"plate\",\"cam_col\"], axis=1)\ncleaned=cleaned.drop(79543)\n# make the X and y varialbes\nX = cleaned.drop('class', axis=1)\ny = cleaned['class']\ndisplay(cleaned)\n\n\nsm = SMOTE(random_state=42)\nX, y = sm.fit_resample(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n\nfrom tensorflow.keras import utils\n# y = utils.to_categorical(y)\n# y_train = utils.to_categorical(y_train)\n# y_test = utils.to_categorical(y_test)\n\n\nCode\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=20, activation=\"tanh\"))\nmodel.add(Dense(units=64,  activation=\"tanh\"))\nmodel.add(Dense(units=32,  activation=\"tanh\"))\nmodel.add(Dense(units=3, activation='softmax'))\n\n# compile the keras model\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=\"rmsprop\",\n    metrics='accuracy'\n)\n# fit the model\n# model.fit(X, y, epochs=20, validation_split=0.2)\nmodel_pipeline = Pipeline(steps=[\n  (\"preprocessor\", preprocessor),\n  (\"model\", model),\n])\nm1=model_pipeline.fit(X_train,y_train, model__epochs=20, model__validation_split=0.2,model__batch_size=32,)\n\n\nCode\npredicted=m1.predict(X_test)\ny_classes = predicted.argmax(axis=-1)\n\n# model_pipeline.transform(X_test)\nprint(classification_report(y_test, y_classes)) \n\n\nCode\n# define loss function\nloss = 'accuracy'\n\n# create 10 fold CV object\nkfold = KFold(n_splits=5, random_state=123, shuffle=True)\n\n# fit model with 10-fold CV\nresults = cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring=loss)\nresults\n\n\nCode\n# hyper_grid = {'n_neighbors': range(2, 26)}\n# grid_search = GridSearchCV(knn, hyper_grid, cv=kfold, scoring=loss)\n# results = grid_search.fit(X_train, y_train)\n\n\n\nk-means clustring | trash\n\nCode\nmodel = sklearn.cluster.KMeans(n_clusters=3,random_state=123)\nmodel\n\n\nCode\ngalaxy = cleaned[cleaned[\"class\"]==0].drop(\"class\",axis=1)\ngalaxy_centers = map(lambda a: a/galaxy.shape[0],galaxy.sum())\ngalaxy_centers= np.array(list(galaxy_centers))\ngalaxy_centers\n\n\nSTAR = cleaned[cleaned[\"class\"]==1].drop(\"class\",axis=1)\nSTAR_centers = map(lambda a: a/STAR.shape[0],STAR.sum())\nSTAR_centers= np.array(list(STAR_centers))\nSTAR_centers\n\n\n\nQSO = cleaned[cleaned[\"class\"]==2].drop(\"class\",axis=1)\nQSO_centers = map(lambda a: a/QSO.shape[0],QSO.sum())\nQSO_centers= np.array(list(QSO_centers))\nQSO_centers\n\n\n\nCode\nm1=model.fit([galaxy_centers,STAR_centers,QSO_centers])\nm1.labels_\n\n\nCode\npred= m1.predict(X_test)\nprint(classification_report(y_test, pred))"
  }
]